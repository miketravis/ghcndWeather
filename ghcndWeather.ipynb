{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining Daily Historical Weather Data from the Global Historical Climatology Network (GHCN) off of NOAA's Website using Python on IBM's Data Science Experience Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Michael Travis  \n",
    "2017-07-27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides code to retrieve, clean and format the daily historical weather data from the Global Historical Climatology Network (GHCN) off of NOAA's website (ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/).\n",
    "The Code belows grabs the weather data from US weather stations over the past 82 years that have 2% or less missing data. Code can be changed to change location, time frame and amount of missing data.\n",
    "\n",
    "The functions df_ghcnd_inventory(), df_ghcnd_stations(), get_ghcnd_all(), df_ghcnd_all() were heavily derived from the functions in https://github.com/jjrennie/GHCNpy\n",
    "\n",
    "The cleaning and formatting followed http://spatialreasoning.com/wp/20170307_1244_r-reading-filtering-weather-data-from-the-global-historical-climatology-network-ghcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Station Metadata  \n",
    "This section will obtain the metadata for all the stations by getting the ghcnd-stations.txt and ghcnd-inventory.txt files from the website.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from ftplib import FTP\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From the readme.txt:  \n",
    "#  \n",
    "#IV. FORMAT OF \"ghcnd-stations.txt\"  \n",
    "#  \n",
    "#------------------------------  \n",
    "#Variable   Columns   Type  \n",
    "#------------------------------  \n",
    "#ID            1-11   Character  \n",
    "#LATITUDE     13-20   Real  \n",
    "#LONGITUDE    22-30   Real  \n",
    "#ELEVATION    32-37   Real  \n",
    "#STATE        39-40   Character  \n",
    "#NAME         42-71   Character  \n",
    "#GSN FLAG     73-75   Character  \n",
    "#HCN/CRN FLAG 77-79   Character  \n",
    "#WMO ID       81-85   Character  \n",
    "#------------------------------\n",
    "#\n",
    "#creates a dataframe for ghcnd-stations.txt from noaa's website\n",
    "def df_ghcnd_stations():\n",
    "    print(\"GRABBING LATEST STATION METADATA FILE\")\n",
    "\n",
    "    ftp = FTP('ftp.ncdc.noaa.gov')\n",
    "    ftp.login()\n",
    "    ftp.cwd('pub/data/ghcn/daily')\n",
    "    ftp.retrbinary('RETR ghcnd-stations.txt', open('ghcnd-stations.txt', 'wb').write)\n",
    "    ftp.quit()\n",
    "\n",
    "    # Read in GHCND-D Stations File\n",
    "    ghcnd_stnfile = 'ghcnd-stations.txt'\n",
    "    ghcnd_stations = np.genfromtxt(ghcnd_stnfile,delimiter=(11,9,10,7,1,2,1,30,1,3,1,3,1,5),dtype=str)\n",
    "    \n",
    "    #creates a dataframe form the data in the text file\n",
    "    dataframe = pd.DataFrame(ghcnd_stations)\n",
    "    \n",
    "    #deletes empty columns\n",
    "    deleteCols = [4,6,8,10,12]\n",
    "    for i in deleteCols:\n",
    "        del dataframe[i]\n",
    "    \n",
    "    #gives columns header names\n",
    "    headers = [\"ID\", \"LAT\", \"LON\", \"ELEV\", \"ST\", \"NAME\",\"GSN\", \"HCN\", \"WMOID\"]\n",
    "    dataframe.columns = headers\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From the readme.txt:  \n",
    "#    \n",
    "#VII. FORMAT OF \"ghcnd-inventory.txt\"  \n",
    "#  \n",
    "#------------------------------  \n",
    "#Variable   Columns   Type  \n",
    "#------------------------------  \n",
    "#ID            1-11   Character  \n",
    "#LATITUDE     13-20   Real  \n",
    "#LONGITUDE    22-30   Real  \n",
    "#ELEMENT      32-35   Character  \n",
    "#FIRSTYEAR    37-40   Integer  \n",
    "#LASTYEAR     42-45   Integer  \n",
    "#------------------------------  \n",
    "#  \n",
    "#ELEMENT    is the element type.   There are five core elements as well as a number\n",
    "#           of addition elements.  \n",
    "#   \n",
    "#           The five core elements are:  \n",
    "#  \n",
    "#           PRCP = Precipitation (tenths of mm)  \n",
    "#           SNOW = Snowfall (mm)  \n",
    "#           SNWD = Snow depth (mm)  \n",
    "#           TMAX = Maximum temperature (tenths of degrees C)  \n",
    "#           TMIN = Minimum temperature (tenths of degrees C)  \n",
    "#         \n",
    "#FIRSTYEAR and LASTYEAR are the first and last years the weather station recorded that element.\n",
    "#\n",
    "#creates a dataframe for ghcnd-inventory.txt from noaa's website\n",
    "def df_ghcnd_inventory():\n",
    "    print(\"GRABBING LATEST STATION INVENTORY FILE\")\n",
    "\n",
    "    ftp = FTP('ftp.ncdc.noaa.gov')\n",
    "    ftp.login()\n",
    "    ftp.cwd('pub/data/ghcn/daily')\n",
    "    ftp.retrbinary('RETR ghcnd-inventory.txt', open('ghcnd-inventory.txt', 'wb').write)\n",
    "    ftp.quit()\n",
    "\n",
    "    # Read in GHCND-D INVENTORY File\n",
    "    ghcnd_invfile='ghcnd-inventory.txt'\n",
    "    ghcnd_inventory= np.genfromtxt(ghcnd_invfile,delimiter=(11,1,8,1,9,1,4,1,4,1,4),dtype=str)\n",
    "    \n",
    "    #creates a dataframe form the data in the text file\n",
    "    dataframe = pd.DataFrame(ghcnd_inventory)\n",
    "    \n",
    "    #deletes empty columns\n",
    "    deleteCols = [1,3,5,7,9]\n",
    "    for i in deleteCols:\n",
    "        del dataframe[i]\n",
    "    \n",
    "    #gives columns header names\n",
    "    headers = [\"ID\", \"LAT\", \"LON\", \"ELEM\" , \"FIRST\", \"LAST\"]\n",
    "    dataframe.columns = headers\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRABBING LATEST STATION METADATA FILE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>ELEV</th>\n",
       "      <th>ST</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSN</th>\n",
       "      <th>HCN</th>\n",
       "      <th>WMOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>10.1</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS COOLIDGE FLD</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td>17.1333</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>19.2</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.5170</td>\n",
       "      <td>34.0</td>\n",
       "      <td></td>\n",
       "      <td>SHARJAH INTER. AIRP</td>\n",
       "      <td>GSN</td>\n",
       "      <td></td>\n",
       "      <td>41196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.3640</td>\n",
       "      <td>10.4</td>\n",
       "      <td></td>\n",
       "      <td>DUBAI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>41194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.4330</td>\n",
       "      <td>54.6510</td>\n",
       "      <td>26.8</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>41217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID        LAT         LON     ELEV  ST  \\\n",
       "0  ACW00011604    17.1167    -61.7833     10.1       \n",
       "1  ACW00011647    17.1333    -61.7833     19.2       \n",
       "2  AE000041196    25.3330     55.5170     34.0       \n",
       "3  AEM00041194    25.2550     55.3640     10.4       \n",
       "4  AEM00041217    24.4330     54.6510     26.8       \n",
       "\n",
       "                             NAME  GSN  HCN  WMOID  \n",
       "0  ST JOHNS COOLIDGE FLD                            \n",
       "1  ST JOHNS                                         \n",
       "2  SHARJAH INTER. AIRP             GSN       41196  \n",
       "3  DUBAI INTL                                41194  \n",
       "4  ABU DHABI INTL                            41217  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stns = df_ghcnd_stations()\n",
    "df_stns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRABBING LATEST STATION INVENTORY FILE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>ELEM</th>\n",
       "      <th>FIRST</th>\n",
       "      <th>LAST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID       LAT        LON  ELEM FIRST  LAST\n",
       "0  ACW00011604   17.1167   -61.7833  TMAX  1949  1949\n",
       "1  ACW00011604   17.1167   -61.7833  TMIN  1949  1949\n",
       "2  ACW00011604   17.1167   -61.7833  PRCP  1949  1949\n",
       "3  ACW00011604   17.1167   -61.7833  SNOW  1949  1949\n",
       "4  ACW00011604   17.1167   -61.7833  SNWD  1949  1949"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inv = df_ghcnd_inventory()\n",
    "df_inv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove spaces from the LAT and LON columns and convert to float\n",
    "cols = [\"LAT\", \"LON\"]\n",
    "for i in cols:\n",
    "    df_inv[i] = df_inv.apply(lambda row: float(row[i].strip()),axis=1)\n",
    "\n",
    "#remove spaces from the FIRST and LAST columns and convert to int\n",
    "cols = [\"FIRST\", \"LAST\"]\n",
    "for i in cols:\n",
    "    df_inv[i] = df_inv.apply(lambda row: int(row[i].strip()),axis=1)\n",
    "\n",
    "#remove spaces from the ID and ELEM columns\n",
    "cols = [\"ID\", \"ELEM\"]\n",
    "for i in cols:\n",
    "    df_inv[i] = df_inv.apply(lambda row: row[i].strip(),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Subsetting to Stations of Interest  \n",
    "This section subsets and merges the df_inv and df_stns dataframes to stations in the US that conatin max and min temperature and precipitation data for at least the past 82 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Subset to only stations in the US\n",
    "us1 = df_inv[df_inv[\"ID\"].str.contains('US')]\n",
    "\n",
    "#Drop the LAT and LON columns as the inventory and stations dataframes will be merged below \n",
    "#and the stations dataframe already contains LAT and LON\n",
    "del us1[\"LAT\"]\n",
    "del us1[\"LON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Subset to stations that have been monitoring max temperature since 1935 or earlier\n",
    "us2 = us1[(us1.ELEM == 'TMAX') & (us1.FIRST <= 1935) & (us1.LAST >=2017)]\n",
    "del us2[\"ELEM\"]\n",
    "headers = (\"ID\", \"TMAXf\", \"TMAXl\")\n",
    "us2.columns = headers\n",
    "\n",
    "#Subset to stations that have been monitoring min temperature since 1935 or earlier\n",
    "us3 = us1[(us1.ELEM == 'TMIN') & (us1.FIRST <= 1935) & (us1.LAST >=2017)]\n",
    "del us3[\"ELEM\"]\n",
    "headers = (\"ID\", \"TMINf\", \"TMINl\")\n",
    "us3.columns = headers\n",
    "\n",
    "#Subset to stations that have been monitoring precipitation since 1935 or earlier\n",
    "us4 = us1[(us1.ELEM == 'PRCP') & (us1.FIRST <= 1935) & (us1.LAST >=2017)]\n",
    "del us4[\"ELEM\"]\n",
    "headers = (\"ID\", \"PRCPf\", \"PRCPl\")\n",
    "us4.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_years = us2.merge(us3, how = 'inner', on = 'ID')\n",
    "us_years = us_years.merge(us4, how = 'inner', on = 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TMAXf</th>\n",
       "      <th>TMAXl</th>\n",
       "      <th>TMINf</th>\n",
       "      <th>TMINl</th>\n",
       "      <th>PRCPf</th>\n",
       "      <th>PRCPl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00010178</td>\n",
       "      <td>1934</td>\n",
       "      <td>2017</td>\n",
       "      <td>1934</td>\n",
       "      <td>2017</td>\n",
       "      <td>1934</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00010252</td>\n",
       "      <td>1912</td>\n",
       "      <td>2017</td>\n",
       "      <td>1912</td>\n",
       "      <td>2017</td>\n",
       "      <td>1912</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00010583</td>\n",
       "      <td>1915</td>\n",
       "      <td>2017</td>\n",
       "      <td>1915</td>\n",
       "      <td>2017</td>\n",
       "      <td>1913</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00012758</td>\n",
       "      <td>1890</td>\n",
       "      <td>2017</td>\n",
       "      <td>1890</td>\n",
       "      <td>2017</td>\n",
       "      <td>1890</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00012813</td>\n",
       "      <td>1917</td>\n",
       "      <td>2017</td>\n",
       "      <td>1917</td>\n",
       "      <td>2017</td>\n",
       "      <td>1917</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  TMAXf  TMAXl  TMINf  TMINl  PRCPf  PRCPl\n",
       "0  USC00010178   1934   2017   1934   2017   1934   2017\n",
       "1  USC00010252   1912   2017   1912   2017   1912   2017\n",
       "2  USC00010583   1915   2017   1915   2017   1913   2017\n",
       "3  USC00012758   1890   2017   1890   2017   1890   2017\n",
       "4  USC00012813   1917   2017   1917   2017   1917   2017"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The resulting dataframe looks like this\n",
    "us_years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Subset to only stations in the US\n",
    "us_stns = df_stns[df_stns[\"ID\"].str.contains('US')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SD', 'CO', 'NE', 'AK', 'AL', 'AR', 'AZ', 'CA', 'TN', 'CT', 'DC',\n",
       "       'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA',\n",
       "       'MA', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NH',\n",
       "       'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'TX',\n",
       "       'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY', 'PI', 'UM'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_stns['ST'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Manually remove leftover stations that aren't in the US\n",
    "conus_stns = us_stns[(us_stns.ST != \"AK\") & (us_stns.ST != \"HI\") & (us_stns.ST != \"PI\") & (us_stns.ST != \"UM\")]\n",
    "us82 = us_stns.merge(us_years, how = 'right', on = 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>ELEV</th>\n",
       "      <th>ST</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSN</th>\n",
       "      <th>HCN</th>\n",
       "      <th>WMOID</th>\n",
       "      <th>TMAXf</th>\n",
       "      <th>TMAXl</th>\n",
       "      <th>TMINf</th>\n",
       "      <th>TMINl</th>\n",
       "      <th>PRCPf</th>\n",
       "      <th>PRCPl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00010178</td>\n",
       "      <td>33.1272</td>\n",
       "      <td>-88.1550</td>\n",
       "      <td>59.4</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALICEVILLE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1934</td>\n",
       "      <td>2017</td>\n",
       "      <td>1934</td>\n",
       "      <td>2017</td>\n",
       "      <td>1934</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00010252</td>\n",
       "      <td>31.3072</td>\n",
       "      <td>-86.5225</td>\n",
       "      <td>76.2</td>\n",
       "      <td>AL</td>\n",
       "      <td>ANDALUSIA 3 W</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1912</td>\n",
       "      <td>2017</td>\n",
       "      <td>1912</td>\n",
       "      <td>2017</td>\n",
       "      <td>1912</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00010583</td>\n",
       "      <td>30.8839</td>\n",
       "      <td>-87.7853</td>\n",
       "      <td>82.6</td>\n",
       "      <td>AL</td>\n",
       "      <td>BAY MINETTE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1915</td>\n",
       "      <td>2017</td>\n",
       "      <td>1915</td>\n",
       "      <td>2017</td>\n",
       "      <td>1913</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00012758</td>\n",
       "      <td>31.4450</td>\n",
       "      <td>-86.9533</td>\n",
       "      <td>88.4</td>\n",
       "      <td>AL</td>\n",
       "      <td>EVERGREEN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1890</td>\n",
       "      <td>2017</td>\n",
       "      <td>1890</td>\n",
       "      <td>2017</td>\n",
       "      <td>1890</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00012813</td>\n",
       "      <td>30.5467</td>\n",
       "      <td>-87.8808</td>\n",
       "      <td>7.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>FAIRHOPE 2 NE</td>\n",
       "      <td></td>\n",
       "      <td>HCN</td>\n",
       "      <td></td>\n",
       "      <td>1917</td>\n",
       "      <td>2017</td>\n",
       "      <td>1917</td>\n",
       "      <td>2017</td>\n",
       "      <td>1917</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID        LAT         LON     ELEV  ST  \\\n",
       "0  USC00010178    33.1272    -88.1550     59.4  AL   \n",
       "1  USC00010252    31.3072    -86.5225     76.2  AL   \n",
       "2  USC00010583    30.8839    -87.7853     82.6  AL   \n",
       "3  USC00012758    31.4450    -86.9533     88.4  AL   \n",
       "4  USC00012813    30.5467    -87.8808      7.0  AL   \n",
       "\n",
       "                             NAME  GSN  HCN  WMOID  TMAXf  TMAXl  TMINf  \\\n",
       "0  ALICEVILLE                                        1934   2017   1934   \n",
       "1  ANDALUSIA 3 W                                     1912   2017   1912   \n",
       "2  BAY MINETTE                                       1915   2017   1915   \n",
       "3  EVERGREEN                                         1890   2017   1890   \n",
       "4  FAIRHOPE 2 NE                        HCN          1917   2017   1917   \n",
       "\n",
       "   TMINl  PRCPf  PRCPl  \n",
       "0   2017   1934   2017  \n",
       "1   2017   1912   2017  \n",
       "2   2017   1913   2017  \n",
       "3   2017   1890   2017  \n",
       "4   2017   1917   2017  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The dataframe contains all US stations that have been operating for 82 or more years\n",
    "us82.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Daily Weather Files\n",
    "This section obtains all the daily weather files from the website and converts the ones needed to .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From the readme.txt\n",
    "#\n",
    "#III. FORMAT OF DATA FILES (\".dly\" FILES)\n",
    "#\n",
    "#Each \".dly\" file contains data for one station.  The name of the file\n",
    "#corresponds to a station's identification code.  For example, \"USC00026481.dly\"\n",
    "#contains the data for the station with the identification code USC00026481).\n",
    "#\n",
    "#Each record in a file contains one month of daily data.  The variables on each\n",
    "#line include the following:\n",
    "#\n",
    "#------------------------------\n",
    "#Variable   Columns   Type\n",
    "#------------------------------\n",
    "#ID            1-11   Character\n",
    "#YEAR         12-15   Integer\n",
    "#MONTH        16-17   Integer\n",
    "#ELEMENT      18-21   Character\n",
    "#VALUE1       22-26   Integer\n",
    "#MFLAG1       27-27   Character\n",
    "#QFLAG1       28-28   Character\n",
    "#SFLAG1       29-29   Character\n",
    "#VALUE2       30-34   Integer\n",
    "#MFLAG2       35-35   Character\n",
    "#QFLAG2       36-36   Character\n",
    "#SFLAG2       37-37   Character\n",
    "#  .           .          .\n",
    "#  .           .          .\n",
    "#  .           .          .\n",
    "#VALUE31    262-266   Integer\n",
    "#MFLAG31    267-267   Character\n",
    "#QFLAG31    268-268   Character\n",
    "#SFLAG31    269-269   Character\n",
    "#------------------------------\n",
    "#\n",
    "#retrieves ghcnd_all.tar.gz from noaa's website\n",
    "def get_ghcnd_all():\n",
    "    print(\"GRABBING ALL STATION DATA\")\n",
    "\n",
    "    ftp = FTP('ftp.ncdc.noaa.gov')\n",
    "    ftp.login()\n",
    "    ftp.cwd('pub/data/ghcn/daily')\n",
    "    ftp.retrbinary('RETR ghcnd_all.tar.gz', open('ghcnd_all.tar.gz', 'wb').write)\n",
    "    ftp.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converts all .dly files to .csv from ghcnd_all.tar\n",
    "def csv_ghcnd_all(ID):\n",
    "    for i in range(len(ID)):\n",
    "        \n",
    "        # Read in all daily files\n",
    "        delimit = (11,4,2,4) + (5,1,1,1)*31\n",
    "        df = np.genfromtxt((\"ghcnd_all/\"+ID[i]+\".dly\"),delimiter=delimit,dtype=str, missing_values = \"-9999\")\n",
    "        \n",
    "        #convert to Pandas Dataframe\n",
    "        dataframe = pd.DataFrame(df)\n",
    "        \n",
    "        #xx headers denote columns that will be dropped\n",
    "        headers = ()\n",
    "        for j in range(1,32):\n",
    "            headers = headers + (\"Val{}\".format(j),\"xx\",\"xx\",\"xx\")\n",
    "    \n",
    "        headers = (\"ID\", \"year\", \"month\", \"element\") + headers\n",
    "        dataframe.columns = headers\n",
    "        \n",
    "        #Drop unneeded columns\n",
    "        del dataframe[\"xx\"]\n",
    "    \n",
    "        dataframe.to_csv((\"ghcnd_all/\"+ID[i]+\".csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Uncomment the code in the four cells below and run each only once.__  \n",
    "The first, third and fourth blocks below will take a very long time to run, i.e. hours. Running the code multiple times will import files that are already imported and convert files to csv that have already been converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_ghcnd_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check the size the file will be after unzipped\n",
    "#!gunzip -c ghcnd_all.tar.gz | wc --bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unzip file\n",
    "#!tar xzvf ghcnd_all.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#csv_ghcnd_all(us82.ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtering Staions with Missing Data\n",
    "This section filters out the stations that are missing greater than 2% of their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Counts how many NA values there are for each weather station for each weather value\n",
    "us82[\"natmax\"] = 0\n",
    "us82[\"natmin\"] = 0\n",
    "us82[\"naprcp\"] = 0\n",
    "for i in range(len(us82[\"ID\"])):\n",
    "    df = pd.read_csv(\"ghcnd_all/\"+us82.ID[i]+\".csv\")\n",
    "    df = df.replace(-9999,np.NaN)\n",
    "    us82.loc[i,\"natmax\"] = sum(df[df.element == 'TMAX'].iloc[:,4:35].isnull().sum(axis=1).tolist())\n",
    "    us82.loc[i,\"natmin\"] = sum(df[df.element == 'TMIN'].iloc[:,4:35].isnull().sum(axis=1).tolist())\n",
    "    us82.loc[i,'naprcp'] = sum(df[df.element == 'PRCP'].iloc[:,4:35].isnull().sum(axis=1).tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>ELEV</th>\n",
       "      <th>ST</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSN</th>\n",
       "      <th>HCN</th>\n",
       "      <th>WMOID</th>\n",
       "      <th>TMAXf</th>\n",
       "      <th>TMAXl</th>\n",
       "      <th>TMINf</th>\n",
       "      <th>TMINl</th>\n",
       "      <th>PRCPf</th>\n",
       "      <th>PRCPl</th>\n",
       "      <th>natmax</th>\n",
       "      <th>natmin</th>\n",
       "      <th>naprcp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>USC00026242</td>\n",
       "      <td>31.9353</td>\n",
       "      <td>-109.2186</td>\n",
       "      <td>1664.2</td>\n",
       "      <td>AZ</td>\n",
       "      <td>PARADISE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1906</td>\n",
       "      <td>2017</td>\n",
       "      <td>1906</td>\n",
       "      <td>2017</td>\n",
       "      <td>1906</td>\n",
       "      <td>2017</td>\n",
       "      <td>440</td>\n",
       "      <td>472</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>USC00029464</td>\n",
       "      <td>33.7478</td>\n",
       "      <td>-112.5983</td>\n",
       "      <td>509.0</td>\n",
       "      <td>AZ</td>\n",
       "      <td>WITTMANN 4SW</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1930</td>\n",
       "      <td>2017</td>\n",
       "      <td>1930</td>\n",
       "      <td>2017</td>\n",
       "      <td>1923</td>\n",
       "      <td>2017</td>\n",
       "      <td>394</td>\n",
       "      <td>400</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>USC00057460</td>\n",
       "      <td>38.4039</td>\n",
       "      <td>-106.4236</td>\n",
       "      <td>2578.6</td>\n",
       "      <td>CO</td>\n",
       "      <td>SARGENTS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1900</td>\n",
       "      <td>2017</td>\n",
       "      <td>1900</td>\n",
       "      <td>2017</td>\n",
       "      <td>1899</td>\n",
       "      <td>2017</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>USC00102667</td>\n",
       "      <td>42.4261</td>\n",
       "      <td>-112.1253</td>\n",
       "      <td>1482.9</td>\n",
       "      <td>ID</td>\n",
       "      <td>DOWNEY</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1895</td>\n",
       "      <td>2017</td>\n",
       "      <td>1895</td>\n",
       "      <td>2017</td>\n",
       "      <td>1895</td>\n",
       "      <td>2017</td>\n",
       "      <td>141</td>\n",
       "      <td>154</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>USC00106424</td>\n",
       "      <td>46.2325</td>\n",
       "      <td>-116.2430</td>\n",
       "      <td>990.0</td>\n",
       "      <td>ID</td>\n",
       "      <td>NEZPERCE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1901</td>\n",
       "      <td>2017</td>\n",
       "      <td>1901</td>\n",
       "      <td>2017</td>\n",
       "      <td>1901</td>\n",
       "      <td>2017</td>\n",
       "      <td>520</td>\n",
       "      <td>519</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID        LAT         LON     ELEV  ST  \\\n",
       "33   USC00026242    31.9353   -109.2186   1664.2  AZ   \n",
       "49   USC00029464    33.7478   -112.5983    509.0  AZ   \n",
       "237  USC00057460    38.4039   -106.4236   2578.6  CO   \n",
       "327  USC00102667    42.4261   -112.1253   1482.9  ID   \n",
       "346  USC00106424    46.2325   -116.2430    990.0  ID   \n",
       "\n",
       "                               NAME  GSN  HCN  WMOID  TMAXf  TMAXl  TMINf  \\\n",
       "33   PARADISE                                          1906   2017   1906   \n",
       "49   WITTMANN 4SW                                      1930   2017   1930   \n",
       "237  SARGENTS                                          1900   2017   1900   \n",
       "327  DOWNEY                                            1895   2017   1895   \n",
       "346  NEZPERCE                                          1901   2017   1901   \n",
       "\n",
       "     TMINl  PRCPf  PRCPl  natmax  natmin  naprcp  \n",
       "33    2017   1906   2017     440     472     470  \n",
       "49    2017   1923   2017     394     400     459  \n",
       "237   2017   1899   2017      36      36     542  \n",
       "327   2017   1895   2017     141     154     268  \n",
       "346   2017   1901   2017     520     519     526  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drops stations with more than 2% of data missing (365*12*31*0.02 = 610.08)\n",
    "stn82 = us82[(us82.natmax<=610) & (us82.natmin<=610) & (us82.naprcp<=610)]\n",
    "stn82.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stn82 now conatains all US stations and their IDS that have more than 98% of their data over the past 82 years. You can now use those IDs to retrieve their respective .csv file and perform exploratory data analysis and modelling on the data. An example of doing analysis on the data can be found at http://spatialreasoning.com/wp/20170316_1035_r-analyzing-weather-data-from-the-global-historical-climatology-network-ghcn, the analysis is in R but can be easily translated to Python and provides an idea of how to perform the analysis with multiple .csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sources  \n",
    "1. Menne, M.J., I. Durre, R.S. Vose, B.E. Gleason, and T.G. Houston, 2012:  An overview \n",
    "of the Global Historical Climatology Network-Daily Database.  Journal of Atmospheric \n",
    "and Oceanic Technology, 29, 897-910, doi:10.1175/JTECH-D-11-00103.1.\n",
    "\n",
    "2. Menne, M.J., I. Durre, B. Korzeniewski, S. McNeal, K. Thomas, X. Yin, S. Anthony, R. Ray, \n",
    "R.S. Vose, B.E.Gleason, and T.G. Houston, 2012: Global Historical Climatology Network - \n",
    "Daily (GHCN-Daily), Version 3.22\n",
    "\n",
    "3. http://spatialreasoning.com/wp/20170307_1244_r-reading-filtering-weather-data-from-the-global-historical-climatology-network-ghcn\n",
    "\n",
    "4. https://github.com/jjrennie/GHCNpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
